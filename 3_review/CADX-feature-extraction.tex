\subsection{\acs*{cadx}: Feature selection and feature extraction} \label{subsec:chp3:img-clas:CADX-fea-ext}
As presented in the previous section, it is a common practise to extract a wide
variety of features.
While dealing with \ac{mpmri}, the feature space created is a high-dimensional
space which might mislead or corrupt the classifier during the training phase.
Therefore, it is of interest to reduce the number of dimensions before
proceeding to the classification task.
The strategies used can be grouped as: (i) feature selection and (ii) feature
extraction.
In this section only the methods used in \ac{cad} for \ac{cap} systems are
presented.

\subsubsection{Feature selection}\label{subsubsec:chp3:img-clas:CADX:fea-ext:sel}
The feature selection strategy is based on selecting the most discriminative
feature dimensions of the high-dimensional space.
Thus, the low-dimensional space is then composed of a subset of the original
features detected.
In this section, methods employed in \ac{cad} for \ac{cap} detection are
presented.
A more extensive review specific to feature selection is available
in~\cite{Saeys2007}.

\citeauthor{Niaf2012} make use of the p-value by using the independent
two-sample t-test with equal mean for each feature
dimension~\cite{Niaf2011,Niaf2012}.
In this statistical test, there are 2 classes: \ac{cap} and healthy tissue.
Hence, for each particular feature, the distribution of each class is
characterized by their means $\bar{X}_1$ and $\bar{X}_2$ and standard deviation
$s_{X_1}$ and $s_{X_2}$.
Therefore, the null hypothesis test is based on the fact that these both
distribution means are equal.
The t-statistic used to verify the null hypothesis is formalized such that:

\begin{eqnarray}
  t & = & \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \ , \label{eq:tstat} \\
  s_{X_1X_2} & = & \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}} \ , \nonumber
\end{eqnarray}

\noindent where $n_1$ and $n_2$ are the number of samples in each class.
From \acs{eq}\,\eqref{eq:tstat}, more the means of the class distribution
diverge, the larger the $t$-statistic $t$ will be, implying that this
particular feature is more relevant and able to make the distinction between
the two classes.

The $p$-value statistic is deduced from the $t$-test and corresponds to the
probability of obtaining such an extreme test assuming that the null hypothesis
is true~\cite{Goodman1999}.
Hence, smaller the $p$-value, the more likely the null hypothesis to be
rejected and more relevant the feature is likely to be.
Finally, the features are ranked and the most significant features are selected.
However, this technique suffers from a main drawback since it assumes that each
feature is independent, which is unlikely to happen and introduces a high
degree of redundancy in the features selected.

\citeauthor{Vos2012} in~\cite{Vos2012} employed a similar feature ranking
approach but make use of the Fisher discriminant ratio to compute the relevance
of each feature dimension.
Taking the aforementioned formulation, the Fisher discriminant ratio is
formalized as the ratio of the interclass variance to the intraclass variance
as:

\begin{equation}
  F_r = \frac{(\bar{X}_1 - \bar{X}_2)^2}{s^{2}_{X_1}+s^{2}_{X_2}} \ .
  \label{eq:fisherratio}
\end{equation}

Therefore, a relevant feature dimension is selected when the interclass
variance is maximum and the intraclass variance in minimum.
Once the features are ordered, the authors select the feature dimensions with
the largest Fisher discriminant ratio.

\citeauthor{Lemaitre2016thesis} used the one-way \ac{anova}
test~\cite{Lemaitre2016thesis}.
This test is based on computing the F-test which is the ratio of the
between-group variability over the with-in group variability.
The F-value is computed for each pair of features and the $K$ feature
dimensions corresponding to the largest F-values are kept.

\Ac{mi} is a possible metric to use for selecting a subset of feature dimensions.
This method has previously been presented in
\acs{sec}\,\ref{subsec:chp3:img-reg:reg} and expressed in
\acs{eq}\,\eqref{eq:midef}.
\citeauthor{Peng2005}~\cite{Peng2005} introduced two main criteria to select
the feature dimensions based on \ac{mi}: (i) maximal relevance and (ii) minimum
redundancy.
Maximal relevance criterion is based on the paradigm that the classes and the
feature dimension which has to be selected have to share a maximal \ac{mi} and
is formalized as:
\begin{equation}
  \argmax Rel(\mathbf{x},c) = \frac{1}{|\mathbf{x}|} \sum_{x_i \in \mathbf{x}} MI(x_i,c)  \ ,
  \label{eq:mRel}
\end{equation}
\noindent where $\mathbf{x} = \{x_i; i=1,\cdots,d\}$ is a feature vector of $d$
dimensions and $c$ is the class considered.
As in the previous method, using maximal relevance criterion alone imply an
independence between each feature dimension.
The minimal redundancy criterion enforce the selection of a new feature
dimension which shares as little as possible \ac{mi} with the previously
selected feature dimensions such that:
\begin{equation}
  \argmin Red(\mathbf{x}) = \frac{1}{|\mathbf{x}|^2} \sum_{x_i,x_j \in \mathbf{x}} MI(x_i,x_j)  \ .
  \label{eq:mRed}
\end{equation}
Combination of these two criteria is known as the \ac{mrmr}
algorithm~\cite{Peng2005}.
Two combinations are usually used: (i) the difference or (ii) the quotient.
This method has been used at several occasions for the selecting a subset of
features prior to
classification~\cite{Niaf2011,Niaf2012,lehaire2014computer,Viswanath2012,khalvati2015automated,chung2015prostate}.

\Ac{rf} provides information regarding the importance of each feature using the
metric which is based on the Gini index.
The feature importance in \ac{rf} is linked with the Gini importance.
In a tree classifier, the Gini impurity criterion of the child nodes is
inferior to the parent node.
For each individual feature, adding the decrease of the Gini impurity along the
tree gives information about the feature importance: the higher, the better.
Therefore, one can add the decrease of the Gini impurity across all the trees
of a forest and obtain the importance of a specific feature for this forest.
Subsequently, the $K$ most important features are selected to perform the
feature selection.
\citeauthor{Lemaitre2016thesis} used this approach to make a feature selection
when training their \ac{rf}~\cite{Lemaitre2016thesis}.


\subsubsection{Feature extraction}\label{subsubsec:chp3:img-clas:CADX:fea-ext:ext}
The feature extraction strategy is related to dimension reduction methods but
not selecting discriminative features.
Instead, these methods aim at mapping the data from the high-dimensional space
into a low-dimensional space to maximize the separability between the classes.
As in the previous sections, only methods employed in \ac{cad} system are
reviewed in this section.
We refer the reader to~\cite{Fodor2002} for a full review of feature extraction
techniques.

\ac{pca} is the most commonly used linear mapping method in \ac{cad} systems.
\ac{pca} is based on finding the orthogonal linear transform mapping the
original data into a low-dimensional space.
The space is defined such that the linear combinations of the original data
with the $k^{th}$ greatest variances lie on the $k^{th}$ principal
components~\cite{Jolliffe2002}.
The principal components are computed by using the eigenvectors-eigenvalues
decomposition of the covariance matrix.
Let $\mathbf{x}$ denote the data matrix.
Then, the covariance matrix and eigenvectors-eigenvalues decomposition are
defined as in \acs{eq}\,\eqref{eq:covmat}, and \acs{eq}\,\eqref{eq:eigpca},
respectively.
The eigenvectors-eigenvalues decomposition can be formalized as:
\begin{equation}
  \Sigma = \mathbf{x}^{\text{T}} \mathbf{x} \ .
  \label{eq:covmat}
\end{equation}

\begin{equation}
  \mathbf{v}^{-1} \Sigma \mathbf{v} = \Lambda \ ,
  \label{eq:eigpca}
\end{equation}
\noindent where $\mathbf{v}$ are the eigenvectors matrix and $\Lambda$ is a
diagonal matrix containing the eigenvalues.

It is then possible to find the new low-dimensional space by sorting the
eigenvectors using the eigenvalues and finally select the eigenvectors
corresponding to the largest eigenvalues.
The total variation that is the sum of the principal eigenvalues of the
covariance matrix~\cite{Fodor2002}, usually corresponds to the
\SIrange{95}{98}{\percent} of the cumulative sum of the eigenvalues.
\citeauthor{Tiwari2012} used \ac{pca} in order to reduce the complexity of
feature space~\cite{Tiwari2008,Tiwari2009,Tiwari2012,Lemaitre2016thesis}.

Sparse-\ac{pca} is another approach for feature extraction and dimension
reduction~\cite{zou2006sparse}.
Similarly to \ac{pca}, this approach projects the data as a linear combination
of input data.
However, instead of using original data, it uses a sparse representation of the
data, and therefore projects them as linear combination of few input components
rather than all of them.
Referring to \acs{eq}\,\eqref{eq:eigpca}, the cost function of sparse-\ac{pca}
is formulated to maximize the variance while maintaining the sparsity
constraint:

\begin{eqnarray}
 && \argmax \quad   \mathbf{v}^{-1} \Sigma \mathbf{v}\ , \label{eq:sparsepca}\\
 && \text{subject to }  \Vert \mathbf{v} \Vert_{2} = 1\ , \nonumber \\
 && \Vert \mathbf{v} \Vert_{0} \leq k\ . \nonumber
\end{eqnarray}
\noindent where $k$ indicates that number of non-zero elements in $\mathbf{v}$.

\citeauthor{Lemaitre2016thesis} used this approach to decompose the signal
obtained from the \ac{dce}-\ac{mri} and \ac{mrsi} acquisitions.

Similarly to \ac{pca} decomposition, \ac{ica} is projecting data on independent
components~\cite{comon1994independent}.
However, it does not require orthogonality of the space and does not assume
Gaussian distribution for each independent source.
Therefore, opposite to \ac{pca} it can recover uniquely the signals themselves
rather than linear subspace in which the signals lie~\cite{murphy2012machine}.
This method has been used by
\citeauthor{Lemaitre2016thesis}~\cite{Lemaitre2016thesis}.

Non-linear mapping has been also used for dimension reduction and is mainly
based on Laplacian eigenmaps and \acf{lle} methods.
Laplacian eigenmaps also referred as spectral clustering in computer vision,
aim to find a low-dimensional space in which the proximity of the data should
be preserved from the high-dimensional space~\cite{Shi2000,Belkin2001}.
Therefore, two adjacent data points in the high-dimensional space should also
be close in the low-dimensional space.
Similarly, two distant data points in the high-dimensional space should also be
distant in the low-dimensional space.
To compute this projection, an adjacency matrix is defined as:
\begin{equation}
  W(i,j) = \exp \| \mathbf{x}_i - \mathbf{x}_j \|_2 \ ,
  \label{eq:gew}
\end{equation}

\noindent where $\mathbf{x}_i$ and $\mathbf{x}_j$ are the two samples considered.
Then, the low-dimensional space is found by solving the generalized
eigenvectors-eigenvalues problem:

\begin{equation}
  (D-W)\mathbf{y} = \lambda D \mathbf{y} \ ,
  \label{eq:geeig}
\end{equation}

\noindent where $D$ is a diagonal matrix such that $D(i,i) = \sum_j W(j,i)$.
Finally the low-dimensional space is defined by the $k$ eigenvectors of the $k$
smallest eigenvalues~\cite{Belkin2001}.
\citeauthor{Tiwari2009a}~\cite{Tiwari2007,Tiwari2009,Tiwari2009a} and
\citeauthor{Viswanath2008}~\cite{Viswanath2008} used this spectral clustering
to project their feature vector into a low-dimensional space.
The feature space in these studies is usually composed of features extracted
from a single or multiple modalities and then concatenated before applying the
Laplacian eigenmaps dimension reduction technique.

\citeauthor{Tiwari2013} used a slightly different approach by combining the
Laplacian eigenmaps techniques with a prior multi-kernel learning
strategy~\cite{Tiwari2009,Tiwari2013}.
First, multiple features are extracted from multiple modalities.
The features of a single modality are then mapped to a higher-dimensional space
via the Kernel trick~\cite{Aizerman1964}, namely a Gaussian kernel.
Then, each kernel is linearly combined to obtain a combined kernel $K$ and the
adjacency matrix $W$ is computed.
Finally, the same scheme as in the Laplacian eigenmaps is applied.
However, in order to use the combined kernel, \acs{eq}\,\eqref{eq:geeig} is
rewritten as:

\begin{equation}
  K (D-W) K^{\text{T}} \mathbf{y} = \lambda K D K^{\text{T}} \mathbf{y} \ ,
  \label{eq:sesmik}
\end{equation}
\noindent which is solved as a generalized eigenvectors-eigenvalues problem as
previously.
\citeauthor{Viswanath2011} used Laplacian eigenmaps inside a bagging framework
in which multiple embeddings are generated by successively selecting feature
dimensions~\cite{Viswanath2011}.

\Ac{lle} is another common non-linear dimension reduction technique widely
used, first proposed in~\cite{Roweis2000}.
\ac{lle} is based on the fact that a data point in the feature space is
characterized by its neighbourhood.
Thus, each data point in the high-dimensional space is transformed to represent
a linear combination of its $k$-nearest neighbours.
This can be expressed as:
\begin{equation}
  \hat{\mathbf{x}}_i = \sum_j W(i,j) \mathbf{x}_j \ ,
  \label{eq:lincomlle}
\end{equation}

\noindent where $\hat{\mathbf{x}}_i$ are the data points estimated using its
neighbouring data points $\mathbf{x}_j$, and $W$ is the weight matrix.
The weight matrix $W$ is estimated using a least square optimization as in
\acs{eq}\,\eqref{eq:lslle}.
\begin{eqnarray}
  \hat{W} & = & \argmin_{W} \sum_i | \mathbf{x}_i - \sum_j W(i,j)\mathbf{x}_j |^{2} \ , \label{eq:lslle} \\
          && \text{subject to } \sum_j W(i,j) = 1 \ , \nonumber
\end{eqnarray}

Then, the essence of \ac{lle} is to project the data into a low-dimensional
space, while retaining the data spatial organization.
Therefore, the projection into the low-dimensional space is tackled as an
optimization problem as:

\begin{equation}
  \hat{\mathbf{y}} = \argmin_{\mathbf{y}} \sum_i | \mathbf{y}_i - \sum_j W(i,j)\mathbf{y}_j |^{2} \ .
  \label{eq:lowprojlle}
\end{equation}

This optimization is solved as an eigenvectors-eigenvalues problem by finding
the $k^{\text{th}}$ eigenvectors corresponding to the $k^{\text{th}}$ smallest
eigenvalues of the sparse matrix $(I-W)^{\text{T}}(I-W)$.

\citeauthor{Tiwari2008} used a modified version of the \ac{lle} algorithm in
which they applied \ac{lle} in a bagging approach with multiple neighbourhood
sizes~\cite{Tiwari2008}.
The different embeddings obtained are then fused using the \ac{ml} estimation.

Another way of reducing the complexity of high-dimensional feature space is to
use the family of so-called dictionary-based methods.
\Ac{scf} representation has become very popular in other computer vision
application and has been used by \citeauthor{lehaire2014computer}
in~\cite{lehaire2014computer}.
The main goal of sparse modeling is to efficiently represent the images as a
linear combination of a few typical patterns, called atoms, selected from a
dictionary.
Sparse coding consists of three main steps: sparse approximation, dictionary
learning, and low-level features projection~\cite{rubinstein2008efficient}.

\emph{Sparse approximation -} Given a dictionary $\mathbf{D} \in \mathbb{R}^{n
  \times K}$ composed of $K$ atoms and an original signal $\mathbf{y} \in
\mathbb{R}^{n}$ --- i.e., one feature vector ---, the sparse approximation
corresponds to find the sparest vector $\mathbf{x} \in \mathbb{R}^{K}$ such
that:

\begin{equation}
  \argmin_{\mathbf{x}}\|\mathbf{y - Dx} \|_{2} \qquad  \text{s.t.} \  \|\mathbf{x}\|_{0} \leq \lambda \, \label{eq:sprapp} \ ,
\end{equation}
\noindent where $\lambda$ is a specified sparsity level.

Solving the above optimization problem is an NP-hard
problem~\cite{elad2010sparse}.
However, approximate solutions are obtained using greedy algorithms such as
\ac{mp}~\cite{mallat1993matching} or
\ac{omp}~\cite{pati1993orthogonal,davis1997adaptive}.

\emph{Dictionary learning -} As stated previously, the sparse approximation is
computed given a specific dictionary $\mathbf{D}$, which involves a learning
stage from a set of training data.
This dictionary is learned using $K$-\acs*{svd} which is a generalized version
of $K$-means clustering and uses \ac{svd}.
The dictionary is built, in an iterative manner by solving the optimization
problem of \acs{eq}\,\eqref{eq:dct}, by alternatively computing the sparse
approximation of $\mathbf{X}$ and the dictionary $\mathbf{D}$.
\begin{equation}
  \argmin_{\mathbf{D,X}} \|\mathbf{Y} - \mathbf{D}\mathbf{X}\|_{2} \qquad  \text{s.t.} \  \|\mathbf{x}_{i}\|_{1} \leq \lambda \,\label{eq:dct} \ ,
\end{equation}
\noindent where $\mathbf{Y}$ is a training set of low-level descriptors,
$\mathbf{X}$ is the associated sparse coded matrix --- i.e., set of high-level
descriptors --- with a sparsity level $\lambda$, and $\mathbf{D}$ is the
dictionary with $K$ atoms.
Given $\mathbf{D}$, $\mathbf{X}$ is computed using the batch-\ac{omp}
algorithm, while given $\mathbf{X}$, $\mathbf{D}$ is sequentially updated, one
atom at a time using \ac{svd}.

\emph{Low-level features projection -} Once the dictionary is learned, each set
of low-level features $\mathbf{F}_{I}$ previously extracted is encoded using
the dictionary $\mathbf{D}$, solving the optimization problem presented in
\acs{eq}\,\eqref{eq:sprapp} such that $\mathbf{F}_{I} \simeq \mathbf{DX}_{I}$.

The \ac{bow} approach offers an alternative method~\cite{Sivic2003} for feature
extraction.
\Ac{bow} was used by \citeauthor{rampun2016computerb}
in~\cite{rampun2015classifying,rampun2016computerb}.
This model represents the features by creating a codebook or visual dictionary,
from the set of low-level features.
The set of low-level features are clustered using \textit{k}-means to create
the dictionary with \textit{k} clusters known as visual words.
Once the codebook is created from the training set, the low-level descriptors
are replaced by their closest word within the codebook.
The final descriptor is a histogram of size \textit{k} which represents the
codebook occurrences for a given mapping.

\subsubsection{Summary}

The feature selection and extraction used in \ac{cad} systems are summarized in
\acs{tab}~\ref{tab:featext}.

\begin{table}
  \caption{Overview of the feature selection and extraction methods used in \acs*{cad} systems.}
  \centering
  \begin{tabular}{l r}
    \toprule
    \textbf{Dimension reduction methods} & \textbf{References} \\
    \midrule
    \textbf{Feature selection:} & \\ \\ [-1.5ex]
    \quad Statistical test & \cite{Niaf2011,Niaf2012,Vos2012,Lemaitre2016thesis} \\
    \quad \ac{mi}-based methods & \cite{Niaf2011,Niaf2012,Vos2008,lehaire2014computer,khalvati2015automated,chung2015prostate,Lemaitre2016thesis} \\
    \quad Correlation-based methods & \cite{rampun2016computer,rampun2015computer} \\ \\ [-1.5ex]
    \textbf{Feature extraction:} & \\ \\ [-1.5ex]
    \quad Linear mapping & \\
    \quad \quad \acs*{pca} & \cite{Tiwari2008,Tiwari2009} \\
    \quad \quad Sparse-\acs*{pca} & \cite{Lemaitre2016thesis} \\
    \quad \quad \acs*{ica} & \cite{Lemaitre2016thesis} \\
    \quad Non-linear mapping & \\
    \quad \quad Laplacian eigenmaps & \cite{Tiwari2007,Tiwari2009a,Tiwari2009,Tiwari2010,Viswanath2008,Viswanath2011} \\
    \quad \quad \acs*{lle} and \acs*{lle}-based & \cite{Tiwari2008,Tiwari2009,Viswanath2008a,Viswanath2008} \\
    \quad Dictionary-based learning & \\
    \quad \quad Sparse coding & \cite{lehaire2014computer} \\
    \quad \quad \acs*{bow} & \cite{rampun2016computerb,rampun2015classifying} \\
    \bottomrule
  \end{tabular}
  \label{tab:featext}
\end{table}
