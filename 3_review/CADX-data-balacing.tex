\subsection{Feature balancing}\label{subsec:chp6:method:fea-bal}
Data imbalanced is a recurrent issue in classification, notably in medical
data.
The problem of imbalanced dataset lies in the fact that one of the class has a
smallest number of data --- i.e., in medical data, the class corresponding to
patients with a disease --- compared with the other classes.
Therefore, solving the problem of imbalanced is equivalent to under- or
over-sampling part of the dataset to obtain equal number of samples in the
different classes.
Recently, \citeauthor{Lemaitre2016thesis} used balancing methods during the
learning stage to tackle this issue~\cite{imblearn}.

\subsubsection{\Acl*{us1}}
Techniques that reduce the number of samples of the majority class to be equal
to the number of samples of minority class are referred to as \ac{us1}
techniques.

\setenumerate{listparindent=\parindent,itemsep=10px}
\setlist{noitemsep}
\begin{enumerate}[leftmargin=*]

\item[] \textbf{\Ac{nm}} offers three different methods to under-sample the majority
  class~\cite{mani2003knn}.
  In \ac{nm1}, samples from the majority class are selected such that for each
  sample, the average distance to the $k$ \ac{nn} samples from the minority class
  is minimum.
  \ac{nm2} diverges from \ac{nm1} by considering the $k$ farthest neighbours
  samples from the minority class.
  In \ac{nm3}, a subset $M$ containing samples from the majority class is
  generated by finding the $m$ \ac{nn} from each sample of the minority class.
  Then, samples from the subset $M$ are selected such that for each sample, the
  average distance to the $k$ \ac{nn} samples from the minority class is
  maximum.

\item[] \textbf{\Ac{iht}} select samples with a high hardness
  threshold~\cite{smith2014instance}.
  Hardness indicates the likelihood of mis-classification rate for each samples.
  The notation of instance hardness are drawn through the decomposition of $p(h
  \vert t)$ using Bayes' theorem, where $h$ represent the mapping function used
  to map input features to their corresponding labels and $t$ represents the
  training set.
  \begin{equation}
    IH_h(\langle x_{i}, y_{i}\rangle) = 1 - p(y_i \vert x_i, h).\
    \label{eq:iht}
  \end{equation}
  Therefore, under-sampling is performed by keeping the most probable samples ---
  i.e, filtering the samples with high hardness value --- through \ac{kcv}
  training sets while considering specific threshold for filtering.

\end{enumerate}

\subsubsection{\Acl*{os}}
In contrast to \ac{us1} techniques, data can be balanced by \ac{os} in which
the new samples belonging to the minority class are generated, aiming at
equalizing the number of samples in both classes.

\setenumerate{listparindent=\parindent,itemsep=10px}
\setlist{noitemsep}
\begin{enumerate}[leftmargin=*]

\item[] \textbf{\Ac{smote}} is a method to generate new synthetic
  samples~\cite{chawla2002smote}.
  Let define $x_i$ as a sample belonging to the minority class.
  Let define $x_{nn}$ as a randomly selected sample from the $k$-\ac{nn} of
  $x_i$, with $k$ set to 3.
  A new sample $x_j$ is generated such that $x_j = x_i + \sigma \left( x_{nn} -
    x_i \right)$, where $\sigma$ is a random number in the interval
  $\left[0,1\right]$.

\item[] \textbf{\Ac{smoteb1}} over-samples the minority class samples similarly to
  \ac{smote}~\cite{han2005borderline}.
  However, instead of using all the minority samples, it focuses on the
  borderline samples of minority class.
  Borderline samples simply indicate the samples that are closer to the other
  class.
  First, the borderline samples of minority class are detected.
  A sample $x_{i}$ belongs to borderline samples if more than half of its
  $k$-\ac{nn} samples belong to the majority class.
  Synthetic data is then created based on \ac{smote} method for borderline
  samples, by selecting them.
  Then, $s$-\ac{nn} of the minority class are selected to generate synthetic
  sample similarly to \ac{smote}.

\item[] \textbf{\Ac{smoteb2}} performs similarly to \ac{smoteb1}~\cite{han2005borderline}.
  However, the $s$-\ac{nn} are not computed by only considering the minority
  class but by considering both classes.
  The same generation rules as \ac{smote} is used.

\end{enumerate}

\subsubsection{Summary}

The data balancing used in \ac{cad} systems are summarized in
\acs{tab}~\ref{tab:databal}.

\begin{table}
  \caption{Overview of the data balancing methods used in \acs*{cad} systems.}
  \centering
  \begin{tabular}{l r}
    \toprule
    \textbf{Balancing methods} & \textbf{References} \\
    \midrule
    \textbf{Under-sampling:} & \\ \\ [-1.5ex]
    \quad \Acl*{nm1} \& \Acl*{nm2} \& \Acl*{nm3} & \cite{Lemaitre2016thesis} \\
    \quad \Acl*{iht} & \cite{Lemaitre2016thesis} \\
    \textbf{Over-sampling:} & \\ \\ [-1.5ex]
    \quad \acs*{smote} \& \acs*{smoteb1} \& \acs*{smoteb2} & \cite{Lemaitre2016thesis} \\
    \bottomrule
  \end{tabular}
  \label{tab:databal}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../lemaitre_prostate."
%%% End:
